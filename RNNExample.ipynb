{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/098Steve/Jupyter/blob/main/RNNExample.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Building Character-level Language Models in Keras**\n",
        "\n",
        "In this exercise we will explore the simple use case of building a character level language model, much like the auto-correct model we see on word processor applications for many devices. However there will be a difference. We will train our RNN to derive a language model from Shakespeare's Hamlet. Our network will take a sequence of character's from Shakespeare's Hamlet as input and iteratively compute the probability distribution of the next characer to come in the sequence. Let's make some imports and load in the necessary packages."
      ],
      "metadata": {
        "id": "1gKX9Gn-6slO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import callbacks, layers, models"
      ],
      "metadata": {
        "id": "VziNR-OX-ahW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tf.__version__)"
      ],
      "metadata": {
        "id": "ZqmqTdjgx-Lf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import dependencies\n",
        "import sys\n",
        "import numpy as np\n",
        "import re\n",
        "import random\n",
        "import pickle\n",
        "\n",
        "from nltk.corpus import gutenberg\n"
      ],
      "metadata": {
        "id": "Q1r4iycY9GuK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Bidirectional, Dropout\n",
        "from keras.layers import SimpleRNN, GRU, BatchNormalization"
      ],
      "metadata": {
        "id": "0Is47FrT8ToF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.callbacks import LambdaCallback\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "#from keras.utils.data_utils import get_file"
      ],
      "metadata": {
        "id": "v9PxYU4_7gWu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('gutenberg')"
      ],
      "metadata": {
        "id": "2QN7BruLHpvO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ebc10d0a-28b2-475f-b7df-4167973ff3a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/gutenberg.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use the Natural Language Toolkit (NLTK) package in Python to import and preprocess the play, which can be found in the **gutenberg** corpus."
      ],
      "metadata": {
        "id": "2DIVBQDDFYod"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import gutenberg\n",
        "hamlet = gutenberg.words ('shakespeare-hamlet.txt')"
      ],
      "metadata": {
        "id": "4EEvQbTeF9Rj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hamlet[1:100]"
      ],
      "metadata": {
        "id": "KQUGbcHqF2YC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = ' '\n",
        "#For each word\n",
        "for word in hamlet:\n",
        "  #Convert to lower case and add to a string variable\n",
        "  text += str(word).lower()\n",
        "  text += ' '\n",
        "print('Corpus length, Hamlet only:', len(text))"
      ],
      "metadata": {
        "id": "_CpEmxoFH6TE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b64791c-42fe-4799-a87d-a5eae62731f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Corpus length, Hamlet only: 166766\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The string variable (text) now contains the entire sequence of characters that make up the play Hamlet. Now we will proceed and create a vocabulary, or dictionary of characters, for mapping each character to a specific integer. We will create two versions of our dictionary: one with characters mapped to indices, and the other with indices mapped to characters. This is just for the sake of practicality, as we will need both lists for reference.\n"
      ],
      "metadata": {
        "id": "01SMPuFdK3e-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "characters = sorted(list(set(text)))\n",
        "print('Total characters:', len(characters))\n",
        "char_indices = dict((l,i) for i, l in enumerate(characters))\n",
        "indices_char = dict((i,l) for i, l in enumerate(characters))"
      ],
      "metadata": {
        "id": "4DbjsiuGMgJU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ef7d356-abbc-4367-8eb6-e123aa6829db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total characters: 43\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's look at char_indices and indices_char"
      ],
      "metadata": {
        "id": "oIBTr4WEWvnQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "char_indices"
      ],
      "metadata": {
        "id": "jGKwHIep4QgE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "indices_char"
      ],
      "metadata": {
        "id": "eEtLB-hjWqjJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Break text into:\n",
        "Features - Character-level sequences of fixed length,\n",
        "Labels - The next character in sequence"
      ],
      "metadata": {
        "id": "VY75EnO6_V0Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Empty list to collect each sequence\n",
        "training_sequences = []\n",
        "#Empty list to collect next character in sequence\n",
        "next_chars = []\n",
        "#Define length of each input sequence & stride\n",
        "seq_len, stride = 35,1\n",
        "#Loop over text with window of 35 characters, moving 1 stride at a time\n",
        "for i in range (0, len(text)- seq_len, stride):\n",
        "    training_sequences.append(text[i:i+seq_len])\n",
        "    next_chars.append(text[i+seq_len])\n"
      ],
      "metadata": {
        "id": "aDipL-X88nFU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's print out sequences and labels to verify"
      ],
      "metadata": {
        "id": "XDxiBoIBATSZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Print out sequences and labels to verify\n",
        "print('Number of sequences:', len(training_sequences))\n",
        "print('First sequence:', training_sequences[:1])\n",
        "print('Next characters in sequence:', next_chars[:1])\n",
        "print('Second sequences:', training_sequences[1:2])\n",
        "print('Next characters in sequence:', next_chars[1:2]),"
      ],
      "metadata": {
        "id": "XhnV4tCGAmKJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we will vectorise the training data,  For each character in each sequence we will create a vector of length equal to the number of individual characters in the text.  The vector will be a one hot encoded structure will will show the character in its relevant position in the character list.  The character will be represented by 1 and all other vector elements will be 0"
      ],
      "metadata": {
        "id": "Lr_CbvKxDL2h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a matrix of zeros\n",
        "#with dimensions:\n",
        "#(training sequences, length of each sequence, total unique characters)\n",
        "x= np.zeros((len(training_sequences), seq_len, len(characters)), dtype=bool)\n",
        "y= np.zeros((len(training_sequences),  len(characters)), dtype=bool)"
      ],
      "metadata": {
        "id": "SZdLv478D63p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Iterate over training sequences\n",
        "for index, sequence in enumerate(training_sequences):\n",
        "    #iterate over characters per sequence\n",
        "    for sub_index, chars in enumerate(sequence):\n",
        "      #Update character position in feature matrix to 1\n",
        "      x[index, sub_index, char_indices[chars]] =1\n",
        "      #Update character position in label matrix to 1\n",
        "      y[index, char_indices[next_chars[index]]] =1\n",
        "print('Data vectorisation completed,')\n",
        "print('Feature vectors shape',x.shape)\n",
        "print('Label vectors shape',y.shape)"
      ],
      "metadata": {
        "id": "2sIzV58iFKoM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10a5b6e6-84bf-41f5-d486-3d4270276162"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data vectorisation completed,\n",
            "Feature vectors shape (166731, 35, 43)\n",
            "Label vectors shape (166731, 43)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some characters appear more often than others in language. A feature space can be created corresponding to the statistical distribution  of characters over time. The RNN will construct a unique feature space of probability distributions. These are represented by weights and continuously change at successive time steps during the training process. Softmax is a mathematical function that converts a vector of numbers into a vector of probabilities, where the probabilities of each value are proportional to the relative scale of each value in the vector. Sampling is used to select the next character from the probability distributions for posible characters to come. There are different methods of sampling. Greedy sampling is when you choose the character with the highest probability distribution. Controlled randomness (or stochasticity) can be introduced by picking out the next character in a probabilistic manner, rather than a fixed one."
      ],
      "metadata": {
        "id": "nJwlCOobTY3B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Stochastic sampling** One approach could be to reweight the probability distribution of these output values at a given time step. In this manner, we can systematically introduce a little randomness which can be useful in generative modelling. We will implement the controlled introduction of randomness in our sampling strategy by introducing a sampling threshold, which lets us redistribute the Softmax prediction probabilities of our model."
      ],
      "metadata": {
        "id": "GrQWzO46WKoJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sample(softmax_predictions, sample_threshold):\n",
        "  #Make array of predictions, convert to float\n",
        "  softmax_preds = np.asarray(softmax_predictions).astype('float64')\n",
        "  #Log normalise and divide by threshold\n",
        "  log_preds=np.log(softmax_preds)/sample_threshold\n",
        "  #print(\"log_preds=\", log_preds)\n",
        "  #Compute exponential values of log normalized terms\n",
        "  exp_preds = np.exp(log_preds)\n",
        "  #Normalize predictions\n",
        "  norm_preds = exp_preds/np.sum(exp_preds)\n",
        "  #Draw sample from multinomial distribution\n",
        "  prob = np.random.multinomial(1,norm_preds, 1)\n",
        "  #Return max value\n",
        "  return np.argmax(prob)"
      ],
      "metadata": {
        "id": "LnmIwfzQXP70"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The threshold denotes the entropy of the probability distribution we will use to sample a given generation.  A higher threshold will correpond to higher entropy distributions, leading to unreal and unstructured sequences. Lower thresholds, on the other hand, will plainly encode English language representations and morphology, generating familiar words and terms,"
      ],
      "metadata": {
        "id": "UczVpvyBlh-N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we have our training data preprocessed and ready in tensor format, we will experiment with various RNN architectures."
      ],
      "metadata": {
        "id": "kyBWhM_8mP5i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's create some callbacks. Callbacks are a class of functions that allow operations to be performed on our model during the training process. Essentially this function, which will be used in a callback, will take a random sequence of characters from the Hamlet text and then generate 400 characters to follow on starting from the given input.  "
      ],
      "metadata": {
        "id": "lE482SJZnFD-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def on_epoch_end(epoch, _):\n",
        "  global model, model_name\n",
        "  epoch=epoch+1\n",
        "  print('------Generating text after Epoch: %d' %epoch)\n",
        "  #Random index position to start sample input sequence\n",
        "  start_index = random.randint(0, len(text) - seq_len-1)\n",
        "  #End of sequence, corresponding to training sequence length\n",
        "  end_index = start_index + seq_len\n",
        "  #Set up some sampling entropy thresholds\n",
        "  #sampling_range = [0.3, 0.5, 0.7, 1.0, 1.2]\n",
        "  #We will just use 0.3 but others could be used for experimentation\n",
        "  sampling_range = [0.3]\n",
        "  for threshold in sampling_range:\n",
        "    print('\\n-----*Sampling Threshold*:', threshold)\n",
        "    generated =' '\n",
        "    #Take random input sentence\n",
        "    sentence = text[start_index: end_index]\n",
        "    #Add it to 'generated'\n",
        "    generated += sentence\n",
        "    print('Input sentence to generate from:', sentence)\n",
        "    #Print out buffer\n",
        "    sys.stdout.write(generated)\n",
        "    count=35\n",
        "    #Generate next 400 characters in the sequence\n",
        "    for i in range(400):\n",
        "      #Set up matrixfor prediction, initialise with zeros\n",
        "      x_pred = np.zeros((1, seq_len, len(characters)))\n",
        "      for n, char in enumerate(sentence):\n",
        "        x_pred[0,n, char_indices[char]] = 1\n",
        "      #Make prediction on input vector\n",
        "      preds = model.predict(x_pred, verbose=0)[0]\n",
        "      #Get index position of next character using sample function\n",
        "      next_index = sample(preds, threshold)\n",
        "      #Get next character using index\n",
        "      next_char = indices_char[next_index]\n",
        "      #Add generated character to sequence\n",
        "      generated += next_char\n",
        "      sentence = sentence[1:] + next_char\n",
        "      sys.stdout.write(next_char)\n",
        "      count += 1\n",
        "      if(count>120 and next_char ==' '):\n",
        "        count=0\n",
        "        sys.stdout.write('\\n')\n",
        "        sys.stdout.flush()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "x3DV8IPhnuDR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)"
      ],
      "metadata": {
        "id": "q1v3iHkpwRRi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we build a helper function that will train, sample, and save a list of RNN models."
      ],
      "metadata": {
        "id": "mjwa9KZ2V-rF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def test_models(list, epochs=10):\n",
        "  global model, model_name\n",
        "  for network in list:\n",
        "    print('Initiating compilation ....')\n",
        "    #Inialise model\n",
        "    model = network()\n",
        "    #Get model name\n",
        "    model_name = re.split(' ',str(network))[1]\n",
        "    #Filepath to save model with name, epoch and loss\n",
        "    filepath=\"output %s_epoch-{epoch:02d}-loss-{loss:.4f}.keras\"%model_name\n",
        "    print(\"Filepath =\", filepath)\n",
        "    #Checkpoint callback object\n",
        "    checkpoint = ModelCheckpoint(filepath, monitor = 'loss', verbose = 0,\n",
        "                                 save_best_only=True, mode='min')\n",
        "    #Compile model\n",
        "    model.compile(loss='categorical_crossentropy', optimizer ='adam')\n",
        "    print('Compiled:', str(model_name))\n",
        "    #Initiate training\n",
        "    network = model.fit(x,y,batch_size=100, epochs=epochs,\n",
        "                        callbacks=[print_callback, checkpoint])\n",
        "    #Print model configuration\n",
        "    model.summary()\n",
        "    #Save model history object for later analysis\n",
        "    with open(\"history %s.pkl\"%model_name, 'wb') as file_pi:\n",
        "          pickle.dump(network.history, file_pi)\n",
        "\n"
      ],
      "metadata": {
        "id": "jdQ_jNdQzKTK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we will construct several types of RNNs and training them with the helper function to see how different types on RNNs perform at generating Shakespeare-like texts."
      ],
      "metadata": {
        "id": "_6EmDnsv2LJp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Building a SimpleRNN** The SimpleRNN model in Keras is a basic RNN layer. While it has many parameters, most of them are set with excellent defaults that will get you by for many different use cases."
      ],
      "metadata": {
        "id": "nHdemjSFAHEA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def SimpleRNN_model():\n",
        "  model = Sequential()\n",
        "  model.add(SimpleRNN(128, input_shape=(seq_len, len(characters))))\n",
        "  model.add(Dense(len(characters), activation='softmax'))\n",
        "  return model"
      ],
      "metadata": {
        "id": "-UJjDV5qMKJI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def SimpleRNN_stacked_model():\n",
        "  model = Sequential()\n",
        "  model.add(SimpleRNN(128, input_shape=(seq_len, len(characters)),\n",
        "                      return_sequences=True))\n",
        "  model.add(SimpleRNN(128))\n",
        "  model.add(Dense(len(characters), activation='softmax'))\n",
        "  return model\n"
      ],
      "metadata": {
        "id": "AeViPsaYEtsv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"return_sequences =True\" means that the activations of all previous timesteps are input into the next layer. If 'return_sequences' was set to false only the activation weights of the previous time would get passed to the next layer"
      ],
      "metadata": {
        "id": "uRL6cZevHULc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Building GRUs**\n",
        "Now we will build a GRU\n"
      ],
      "metadata": {
        "id": "m9a2KaYWIB2X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def GRU_stacked_model():\n",
        "  model = Sequential()\n",
        "  model.add(GRU(128, input_shape=(seq_len, len(characters)),\n",
        "                return_sequences=True))\n",
        "  model.add(GRU(128))\n",
        "  model.add(Dense(len(characters), activation='softmax'))\n",
        "  return model"
      ],
      "metadata": {
        "id": "sf7pPYRtIs3A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Building a bi-directional GRU** Next we will build a bi-directional GRU which allows the model to learn from previous and future events. We will next the GRU within a bi-directional layer, and feed our model each sequence in both the normal and reverse order."
      ],
      "metadata": {
        "id": "cr81mOs8HIeV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Bi_directional_GRU():\n",
        "  model = Sequential()\n",
        "  model.add(Bidirectional(GRU(128, return_sequences=True),\n",
        "                          input_shape=(seq_len, len(characters))))\n",
        "  model.add(Bidirectional(GRU(128)))\n",
        "  model.add(Dense(len(characters), activation='softmax'))\n",
        "  return model"
      ],
      "metadata": {
        "id": "nl3Nf6nUK87H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Implementing Recurrent Dropout** Dropout is used to randomly drop neurons to better distribute representations over our network and avoid the problem of overfitting. Adding a normal dropout layer doesn't work for RNN and it introduces too much randomness. However the notion of applying the same dropout scheme (or mask) at each time step seems to work. This is one of the most significant techniques that helps overfitting in recurrent layers and is known as a **recurrent dropout strategy**."
      ],
      "metadata": {
        "id": "P_ldMO7TIAEe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def larger_GRU():\n",
        "  model = Sequential()\n",
        "  model.add(GRU(128, input_shape=(seq_len, len(characters)),dropout=0.2,\n",
        "                recurrent_dropout=0.2,return_sequences=True))\n",
        "  model.add(GRU(128, dropout=0.2,recurrent_dropout=0.2))\n",
        "  model.add(Dense(128, activation ='relu'))\n",
        "  model.add(Dense(len(characters), activation='softmax'))\n",
        "  return model"
      ],
      "metadata": {
        "id": "Lzggs9QpJK7S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we'll set up some variables to hold our models. First a variable that is a list of all the models we have built."
      ],
      "metadata": {
        "id": "ZwSK7z16CZht"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#All defined models put as a list - this list could be passed to \"test_models\"\n",
        "#to test all the models in sequence\n",
        "#However it will take too long to train alls model so we wont use this now\n",
        "all_models = [SimpleRNN_model,\n",
        "              SimpleRNN_stacked_model,\n",
        "              GRU_stacked_model,\n",
        "              Bi_directional_GRU,\n",
        "               larger_GRU]"
      ],
      "metadata": {
        "id": "-basfKE-KYbq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we set up five lists each consisting of one model.  This will be better for our initial tests. We can run the models one at a time."
      ],
      "metadata": {
        "id": "E9xYSQu8CvDa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Set up \"lists\" of models.  Here there is one model in each list\n",
        "\n",
        "modelone = [SimpleRNN_model]\n",
        "modeltwo = [SimpleRNN_stacked_model]\n",
        "modelthree =[GRU_stacked_model]\n",
        "modelfour= [Bi_directional_GRU]\n",
        "modelfive = [larger_GRU]"
      ],
      "metadata": {
        "id": "jiWj5zgr1_Eh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's test the first model. There is timing code around the call the test the model which we may wish to use at some point"
      ],
      "metadata": {
        "id": "tPh2f9_bB3Te"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "test_models(modelfive,epochs=10)\n",
        "\n",
        "print(f'Time: {time.time() - start}')"
      ],
      "metadata": {
        "id": "tii7dpZ_Mkn6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#To look at the network history -some data was dumped there. Note you have to use the name of the model you've run.\n",
        "with open(\"//content//history SimpleRNN_model.pkl\", 'rb') as file_pi:\n",
        "         nh= pickle.load(file_pi)\n",
        "nh"
      ],
      "metadata": {
        "id": "HwdIxT0NRKD8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you have time, you can vary the threshold, and you can try some of the other models.  Some of the more complicated models (SimpleRNN_stacked_model, GRU_stacked_model, Bi_directional_GRU,larger_GRU) take a long time to train though.\n"
      ],
      "metadata": {
        "id": "r0jjSuWsAx35"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The main and most important feature of RNN is Hidden state, which remembers some information about a sequence. Recurrent Neural Neworks are a popular algorithm algorithm for sequential data like time series, speech, text, financial data, audio, video, and weather."
      ],
      "metadata": {
        "id": "JSwEBQgEj_N2"
      }
    }
  ]
}