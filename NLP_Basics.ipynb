{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/098Steve/Jupyter/blob/main/NLP_Basics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NLP Basics\n",
        "\n",
        "Go to File -> \"Save a Copy in Drive\"  \n",
        "This allows you to have your own version and edit the file"
      ],
      "metadata": {
        "id": "TNNI5RcyCINm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenisation ‚Äì breaking sentences into words (like breaking a KitKat into sticks)\n",
        "\n",
        "Lowercasing ‚Äì so we don‚Äôt treat ‚ÄúCat‚Äù and ‚Äúcat‚Äù like different species\n",
        "\n",
        "Removing punctuation ‚Äì because commas and full stops don‚Äôt usually help much\n",
        "\n",
        "Removing stop words ‚Äì getting rid of boring words like ‚Äúthe‚Äù and ‚Äúis‚Äù\n",
        "\n",
        "Stemming and Lemmatization ‚Äì turning ‚Äúrunning‚Äù into ‚Äúrun‚Äù, like trimming the fat\n",
        "\n",
        "Putting it all together ‚Äì step-by-step"
      ],
      "metadata": {
        "id": "PUs1noJhCjk8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kpB7atn7CB_4"
      },
      "outputs": [],
      "source": [
        "!pip install nltk\n",
        "!pip install -U spacy\n",
        "!python -m spacy download en_core_web_sm\n",
        "!python -m nltk.downloader punkt stopwords wordnet\n",
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. ‚úÇÔ∏è Tokenisation ‚Äì Chopping up text  \n",
        "Imagine a sentence like a loaf of bread. Tokenisation slices it into manageable chunks (words)."
      ],
      "metadata": {
        "id": "_5xQma4qCp9V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "text = \"Lovely weather we're having today, isn't it?\"\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "print(tokens)"
      ],
      "metadata": {
        "id": "oU0lhpY5Cqvl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. üî° Lowercasing ‚Äì Keeping it simple  \n",
        "Computers are picky. ‚ÄúApple‚Äù and ‚Äúapple‚Äù are not the same to them. Lowercasing helps avoid that."
      ],
      "metadata": {
        "id": "ln6o-mdHCwMg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lower_tokens = [token.lower() for token in tokens]\n",
        "print(lower_tokens)"
      ],
      "metadata": {
        "id": "y0L_Hx0kCyua"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. ‚ùå Remove punctuation ‚Äì Clearing the clutter  \n",
        "Punctuation is like the scaffolding of a sentence. Handy for us, but not so helpful for machines."
      ],
      "metadata": {
        "id": "A2MfzLkYC0lT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "\n",
        "no_punct = [word for word in lower_tokens if word not in string.punctuation]\n",
        "print(no_punct)"
      ],
      "metadata": {
        "id": "oUk_4BOtC3Xh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. üí§ Remove stop words ‚Äì Get to the point  \n",
        "Stop words are the little filler words like \"is\", \"the\", and \"of\". They're like background noise in a conversation."
      ],
      "metadata": {
        "id": "8uM9pi7-C5Yx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_tokens = [word for word in no_punct if word not in stop_words]\n",
        "\n",
        "print(filtered_tokens)"
      ],
      "metadata": {
        "id": "ypOO0OBSC5-k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5a. ü™ì Stemming ‚Äì Chop it off!  \n",
        "Stemming is a bit aggressive. It chops words to their root by brute force. Not always elegant, but gets the job done."
      ],
      "metadata": {
        "id": "VbNkJC0kC_m7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "stemmed = [stemmer.stem(word) for word in filtered_tokens]\n",
        "\n",
        "print(stemmed)"
      ],
      "metadata": {
        "id": "4XYrrphRDCdX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5b. Lemmatization ‚Äì The tidy version  \n",
        "Lemmatization is more thoughtful. It looks at the context and gives you the real base word, or ‚Äúlemma‚Äù."
      ],
      "metadata": {
        "id": "KGpT8OktDELx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
        "\n",
        "print(lemmatized)"
      ],
      "metadata": {
        "id": "BJGh4ISuDIVM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚öôÔ∏è All Together Now: A Clean-up Function  \n",
        "Let‚Äôs now build our ultimate function ‚Äî like a Swiss Army knife of text cleaning üõ†Ô∏è"
      ],
      "metadata": {
        "id": "NiGy8BlJDKNl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text, use_lemmatization=True):\n",
        "    # 1. Tokenise\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # 2. Lowercase\n",
        "    tokens = [word.lower() for word in tokens]\n",
        "\n",
        "    # 3. Remove punctuation\n",
        "    tokens = [word for word in tokens if word not in string.punctuation]\n",
        "\n",
        "    # 4. Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "    # 5. Stemming or Lemmatization\n",
        "    if use_lemmatization:\n",
        "        lemmatizer = WordNetLemmatizer()\n",
        "        tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "    else:\n",
        "        stemmer = PorterStemmer()\n",
        "        tokens = [stemmer.stem(word) for word in tokens]\n",
        "\n",
        "    return tokens\n"
      ],
      "metadata": {
        "id": "HN8b8iMSDMVB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üß™ Let's try it!"
      ],
      "metadata": {
        "id": "t4Tb32OUDS-B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample = \"Cats were running faster than the dogs. Amazing, isn't it?\"\n",
        "print(clean_text(sample))"
      ],
      "metadata": {
        "id": "K2Mlz3HDDTc6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(clean_text(sample, use_lemmatization=False))"
      ],
      "metadata": {
        "id": "jGVQPJOsDVI1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üß™ Let's try it again slightly differently!"
      ],
      "metadata": {
        "id": "9j5MhcGBDo4r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def basic_nlp(text):\n",
        "    # Step 1: Tokenise\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Step 2: Lowercase\n",
        "    lower = [w.lower() for w in tokens]\n",
        "\n",
        "    # Step 3: Remove punctuation\n",
        "    no_punct = [w for w in lower if w not in string.punctuation]\n",
        "\n",
        "    # Step 4: Remove stop words\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    no_stops = [w for w in no_punct if w not in stop_words]\n",
        "\n",
        "    # Step 5: Stemming\n",
        "    stemmer = PorterStemmer()\n",
        "    stems = [stemmer.stem(w) for w in no_stops]\n",
        "\n",
        "    # Step 6: Lemmatization\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmata = [lemmatizer.lemmatize(w) for w in no_stops]\n",
        "\n",
        "    return {\n",
        "        \"original_tokens\": tokens,\n",
        "        \"lowercase\": lower,\n",
        "        \"no_punctuation\": no_punct,\n",
        "        \"no_stopwords\": no_stops,\n",
        "        \"stems\": stems,\n",
        "        \"lemmata\": lemmata\n",
        "    }"
      ],
      "metadata": {
        "id": "dtrMjKk8Dkja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = basic_nlp(\"The quick brown foxes were jumping over the lazy dogs!\")\n",
        "for step, output in result.items():\n",
        "    print(f\"{step}:\\n{output}\\n\")"
      ],
      "metadata": {
        "id": "5-WFW0DVDreU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üïµÔ∏è Named Entity Recognition (NER) ‚Äì Spotting the VIPs  \n",
        "NER is all about picking out the named stuff in a sentence ‚Äî like the nouns that really matter.  \n",
        "  \n",
        "‚ÄúBarack Obama was born in Hawaii.‚Äù  \n",
        "‚Üí \"Barack Obama\" = PERSON  \n",
        "‚Üí \"Hawaii\" = GPE (Geo-Political Entity)  \n",
        "  \n",
        "Think of it like highlighting names, dates and places in a newspaper article ‚Äî but with code.  \n",
        "\n",
        "First, install and load spaCy and its English model (if you haven't already)"
      ],
      "metadata": {
        "id": "SlWBaKAuLC4H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U spacy\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "id": "ggKiLn-cLQby"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üß† NER in action with spaCy"
      ],
      "metadata": {
        "id": "lNNBlxtxLKfl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load the English model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Example sentence\n",
        "text = \"King Charles visited London in April 2023 to attend a meeting with NHS officials.\"\n",
        "\n",
        "# Process the text\n",
        "doc = nlp(text)\n",
        "\n",
        "# Print named entities\n",
        "for ent in doc.ents:\n",
        "    print(f\"{ent.text} ‚Üí {ent.label_}\")"
      ],
      "metadata": {
        "id": "U75nAsp6LVFt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's get info on the NER item\n",
        "spacy.explain(\"GPE\")  # or any other label"
      ],
      "metadata": {
        "id": "OpOjEIfmLdOw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# We can add NER to our pipeline of NLP tools"
      ],
      "metadata": {
        "id": "VUpdaJsxLmFi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def basic_nlp_with_ner(text):\n",
        "    import nltk\n",
        "    import string\n",
        "    import spacy\n",
        "    from nltk.tokenize import word_tokenize\n",
        "    from nltk.corpus import stopwords\n",
        "    from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "\n",
        "    nltk.download('punkt', quiet=True)\n",
        "    nltk.download('stopwords', quiet=True)\n",
        "    nltk.download('wordnet', quiet=True)\n",
        "\n",
        "    # NLTK pipeline\n",
        "    tokens = word_tokenize(text)\n",
        "    lower = [w.lower() for w in tokens]\n",
        "    no_punct = [w for w in lower if w not in string.punctuation]\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    no_stops = [w for w in no_punct if w not in stop_words]\n",
        "    stemmer = PorterStemmer()\n",
        "    stems = [stemmer.stem(w) for w in no_stops]\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmata = [lemmatizer.lemmatize(w) for w in no_stops]\n",
        "\n",
        "    # spaCy NER\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "    doc = nlp(text)\n",
        "    named_entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
        "\n",
        "    return {\n",
        "        \"original_tokens\": tokens,\n",
        "        \"lowercase\": lower,\n",
        "        \"no_punctuation\": no_punct,\n",
        "        \"no_stopwords\": no_stops,\n",
        "        \"stems\": stems,\n",
        "        \"lemmata\": lemmata,\n",
        "        \"named_entities\": named_entities\n",
        "    }"
      ],
      "metadata": {
        "id": "m7j2O6jcLoic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = basic_nlp_with_ner(\"King Charles visited London in April 2023 to attend a meeting with NHS officials.\")\n",
        "for key, value in results.items():\n",
        "    print(f\"{key}:\\n{value}\\n\")"
      ],
      "metadata": {
        "id": "iQWr8G92LtbO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üòÉüòêüò† Sentiment Analysis ‚Äì Reading the mood  \n",
        "We‚Äôll use TextBlob, which is beginner-friendly and built on top of NLTK. It gives us two numbers:  \n",
        "  \n",
        "Polarity (from ‚Äì1 to +1): negative ‚Üí positive  \n",
        "  \n",
        "Subjectivity (from 0 to 1): fact ‚Üí opinion"
      ],
      "metadata": {
        "id": "2fMIsQ7PL44m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "text = \"I absolutely love how easy this is to use!\"\n",
        "blob = TextBlob(text)\n",
        "\n",
        "print(\"Polarity:\", blob.sentiment.polarity)\n",
        "print(\"Subjectivity:\", blob.sentiment.subjectivity)\n"
      ],
      "metadata": {
        "id": "lk7EX251L7hq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Okay, that's a lot of NLP basics!\n",
        "\n",
        "# Now go back through and insert different sentences to see how that changes the output"
      ],
      "metadata": {
        "id": "hhnqosUEMAN-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Once you're satisfied, start looking at tokenisation:\n",
        "https://platform.openai.com/tokenizer  \n",
        "Type sentences into the box, looking at the output"
      ],
      "metadata": {
        "id": "zuhcol6RMRr1"
      }
    }
  ]
}