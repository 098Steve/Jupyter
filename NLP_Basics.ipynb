{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/098Steve/Jupyter/blob/main/NLP_Basics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NLP Basics\n",
        "\n",
        "Go to File -> \"Save a Copy in Drive\"  \n",
        "This allows you to have your own version and edit the file"
      ],
      "metadata": {
        "id": "TNNI5RcyCINm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenisation – breaking sentences into words (like breaking a KitKat into sticks)\n",
        "\n",
        "Lowercasing – so we don’t treat “Cat” and “cat” like different species\n",
        "\n",
        "Removing punctuation – because commas and full stops don’t usually help much\n",
        "\n",
        "Removing stop words – getting rid of boring words like “the” and “is”\n",
        "\n",
        "Stemming and Lemmatization – turning “running” into “run”, like trimming the fat\n",
        "\n",
        "Putting it all together – step-by-step"
      ],
      "metadata": {
        "id": "PUs1noJhCjk8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "kpB7atn7CB_4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1cb50517-1d7c-4430-d4cc-81ea00f4cfe2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.8.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.15.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.11.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.1)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.13.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.1.31)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m98.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "<frozen runpy>:128: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m101.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk\n",
        "!pip install -U spacy\n",
        "!python -m spacy download en_core_web_sm\n",
        "!python -m nltk.downloader punkt stopwords wordnet\n",
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. ✂️ Tokenisation – Chopping up text  \n",
        "Imagine a sentence like a loaf of bread. Tokenisation slices it into manageable chunks (words)."
      ],
      "metadata": {
        "id": "_5xQma4qCp9V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "text = \"Lovely weather we're having today, isn't it?\"\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "print(tokens)"
      ],
      "metadata": {
        "id": "oU0lhpY5Cqvl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf0787ee-5cfa-4d8e-d4f0-91f1f7fb2441"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Lovely', 'weather', 'we', \"'re\", 'having', 'today', ',', 'is', \"n't\", 'it', '?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. 🔡 Lowercasing – Keeping it simple  \n",
        "Computers are picky. “Apple” and “apple” are not the same to them. Lowercasing helps avoid that."
      ],
      "metadata": {
        "id": "ln6o-mdHCwMg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lower_tokens = [token.lower() for token in tokens]\n",
        "print(lower_tokens)"
      ],
      "metadata": {
        "id": "y0L_Hx0kCyua",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6189c10c-e2f3-4d05-9c9e-c67192b7eef1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['lovely', 'weather', 'we', \"'re\", 'having', 'today', ',', 'is', \"n't\", 'it', '?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. ❌ Remove punctuation – Clearing the clutter  \n",
        "Punctuation is like the scaffolding of a sentence. Handy for us, but not so helpful for machines."
      ],
      "metadata": {
        "id": "A2MfzLkYC0lT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "\n",
        "no_punct = [word for word in lower_tokens if word not in string.punctuation]\n",
        "print(no_punct)"
      ],
      "metadata": {
        "id": "oUk_4BOtC3Xh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5320a9e-dbb7-4493-97ca-83c7d3a252e0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['lovely', 'weather', 'we', \"'re\", 'having', 'today', 'is', \"n't\", 'it']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. 💤 Remove stop words – Get to the point  \n",
        "Stop words are the little filler words like \"is\", \"the\", and \"of\". They're like background noise in a conversation."
      ],
      "metadata": {
        "id": "8uM9pi7-C5Yx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_tokens = [word for word in no_punct if word not in stop_words]\n",
        "\n",
        "print(filtered_tokens)"
      ],
      "metadata": {
        "id": "ypOO0OBSC5-k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ffcc7dc-e0f8-485d-e3bf-245aa675c7a2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['lovely', 'weather', \"'re\", 'today', \"n't\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5a. 🪓 Stemming – Chop it off!  \n",
        "Stemming is a bit aggressive. It chops words to their root by brute force. Not always elegant, but gets the job done."
      ],
      "metadata": {
        "id": "VbNkJC0kC_m7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "stemmed = [stemmer.stem(word) for word in filtered_tokens]\n",
        "\n",
        "print(stemmed)"
      ],
      "metadata": {
        "id": "4XYrrphRDCdX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24b311eb-1c30-4846-9789-21694484134d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['love', 'weather', \"'re\", 'today', \"n't\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5b. Lemmatization – The tidy version  \n",
        "Lemmatization is more thoughtful. It looks at the context and gives you the real base word, or “lemma”."
      ],
      "metadata": {
        "id": "KGpT8OktDELx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
        "\n",
        "print(lemmatized)"
      ],
      "metadata": {
        "id": "BJGh4ISuDIVM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60854ccb-3d99-4498-bce1-f0126bc91c02"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['lovely', 'weather', \"'re\", 'today', \"n't\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "⚙️ All Together Now: A Clean-up Function  \n",
        "Let’s now build our ultimate function — like a Swiss Army knife of text cleaning 🛠️"
      ],
      "metadata": {
        "id": "NiGy8BlJDKNl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text, use_lemmatization=True):\n",
        "    # 1. Tokenise\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # 2. Lowercase\n",
        "    tokens = [word.lower() for word in tokens]\n",
        "\n",
        "    # 3. Remove punctuation\n",
        "    tokens = [word for word in tokens if word not in string.punctuation]\n",
        "\n",
        "    # 4. Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "    # 5. Stemming or Lemmatization\n",
        "    if use_lemmatization:\n",
        "        lemmatizer = WordNetLemmatizer()\n",
        "        tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "    else:\n",
        "        stemmer = PorterStemmer()\n",
        "        tokens = [stemmer.stem(word) for word in tokens]\n",
        "\n",
        "    return tokens\n"
      ],
      "metadata": {
        "id": "HN8b8iMSDMVB"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "🧪 Let's try it!"
      ],
      "metadata": {
        "id": "t4Tb32OUDS-B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample = \"Cats were running faster than the dogs. Amazing, isn't it?\"\n",
        "print(clean_text(sample))"
      ],
      "metadata": {
        "id": "K2Mlz3HDDTc6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a005cd06-ff19-455a-d15c-81ed4b97e2b9"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['cat', 'running', 'faster', 'dog', 'amazing', \"n't\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(clean_text(sample, use_lemmatization=False))"
      ],
      "metadata": {
        "id": "jGVQPJOsDVI1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "🧪 Let's try it again slightly differently!"
      ],
      "metadata": {
        "id": "9j5MhcGBDo4r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def basic_nlp(text):\n",
        "    # Step 1: Tokenise\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Step 2: Lowercase\n",
        "    lower = [w.lower() for w in tokens]\n",
        "\n",
        "    # Step 3: Remove punctuation\n",
        "    no_punct = [w for w in lower if w not in string.punctuation]\n",
        "\n",
        "    # Step 4: Remove stop words\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    no_stops = [w for w in no_punct if w not in stop_words]\n",
        "\n",
        "    # Step 5: Stemming\n",
        "    stemmer = PorterStemmer()\n",
        "    stems = [stemmer.stem(w) for w in no_stops]\n",
        "\n",
        "    # Step 6: Lemmatization\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmata = [lemmatizer.lemmatize(w) for w in no_stops]\n",
        "\n",
        "    return {\n",
        "        \"original_tokens\": tokens,\n",
        "        \"lowercase\": lower,\n",
        "        \"no_punctuation\": no_punct,\n",
        "        \"no_stopwords\": no_stops,\n",
        "        \"stems\": stems,\n",
        "        \"lemmata\": lemmata\n",
        "    }"
      ],
      "metadata": {
        "id": "dtrMjKk8Dkja"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = basic_nlp(\"The quick brown foxes were jumping over the lazy dogs!\")\n",
        "for step, output in result.items():\n",
        "    print(f\"{step}:\\n{output}\\n\")"
      ],
      "metadata": {
        "id": "5-WFW0DVDreU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbb367c5-e7d6-46ab-c5ed-724e24451d3f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "original_tokens:\n",
            "['The', 'quick', 'brown', 'foxes', 'were', 'jumping', 'over', 'the', 'lazy', 'dogs', '!']\n",
            "\n",
            "lowercase:\n",
            "['the', 'quick', 'brown', 'foxes', 'were', 'jumping', 'over', 'the', 'lazy', 'dogs', '!']\n",
            "\n",
            "no_punctuation:\n",
            "['the', 'quick', 'brown', 'foxes', 'were', 'jumping', 'over', 'the', 'lazy', 'dogs']\n",
            "\n",
            "no_stopwords:\n",
            "['quick', 'brown', 'foxes', 'jumping', 'lazy', 'dogs']\n",
            "\n",
            "stems:\n",
            "['quick', 'brown', 'fox', 'jump', 'lazi', 'dog']\n",
            "\n",
            "lemmata:\n",
            "['quick', 'brown', 'fox', 'jumping', 'lazy', 'dog']\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "🕵️ Named Entity Recognition (NER) – Spotting the VIPs  \n",
        "NER is all about picking out the named stuff in a sentence — like the nouns that really matter.  \n",
        "  \n",
        "“Barack Obama was born in Hawaii.”  \n",
        "→ \"Barack Obama\" = PERSON  \n",
        "→ \"Hawaii\" = GPE (Geo-Political Entity)  \n",
        "  \n",
        "Think of it like highlighting names, dates and places in a newspaper article — but with code.  \n",
        "\n",
        "First, install and load spaCy and its English model (if you haven't already)"
      ],
      "metadata": {
        "id": "SlWBaKAuLC4H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U spacy\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "id": "ggKiLn-cLQby",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c6f19f7-9b51-4d11-f061-517a16e80dfd"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.8.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.15.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.11.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.1)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.13.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.1.31)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m98.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "🧠 NER in action with spaCy"
      ],
      "metadata": {
        "id": "lNNBlxtxLKfl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load the English model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Example sentence\n",
        "text = \"King Charles visited London in April 2023 to attend a meeting with NHS officials.\"\n",
        "\n",
        "# Process the text\n",
        "doc = nlp(text)\n",
        "\n",
        "# Print named entities\n",
        "for ent in doc.ents:\n",
        "    print(f\"{ent.text} → {ent.label_}\")"
      ],
      "metadata": {
        "id": "U75nAsp6LVFt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99cddc93-9f9d-48de-f7e8-616e1505889a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Charles → PERSON\n",
            "London → GPE\n",
            "April 2023 → DATE\n",
            "NHS → ORG\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's get info on the NER item\n",
        "spacy.explain(\"GPE\")  # or any other label"
      ],
      "metadata": {
        "id": "OpOjEIfmLdOw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "dc637877-168f-44b9-94e9-9c2e1bb9c757"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Countries, cities, states'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# We can add NER to our pipeline of NLP tools"
      ],
      "metadata": {
        "id": "VUpdaJsxLmFi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def basic_nlp_with_ner(text):\n",
        "    import nltk\n",
        "    import string\n",
        "    import spacy\n",
        "    from nltk.tokenize import word_tokenize\n",
        "    from nltk.corpus import stopwords\n",
        "    from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "\n",
        "    nltk.download('punkt', quiet=True)\n",
        "    nltk.download('stopwords', quiet=True)\n",
        "    nltk.download('wordnet', quiet=True)\n",
        "\n",
        "    # NLTK pipeline\n",
        "    tokens = word_tokenize(text)\n",
        "    lower = [w.lower() for w in tokens]\n",
        "    no_punct = [w for w in lower if w not in string.punctuation]\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    no_stops = [w for w in no_punct if w not in stop_words]\n",
        "    stemmer = PorterStemmer()\n",
        "    stems = [stemmer.stem(w) for w in no_stops]\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmata = [lemmatizer.lemmatize(w) for w in no_stops]\n",
        "\n",
        "    # spaCy NER\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "    doc = nlp(text)\n",
        "    named_entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
        "\n",
        "    return {\n",
        "        \"original_tokens\": tokens,\n",
        "        \"lowercase\": lower,\n",
        "        \"no_punctuation\": no_punct,\n",
        "        \"no_stopwords\": no_stops,\n",
        "        \"stems\": stems,\n",
        "        \"lemmata\": lemmata,\n",
        "        \"named_entities\": named_entities\n",
        "    }"
      ],
      "metadata": {
        "id": "m7j2O6jcLoic"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = basic_nlp_with_ner(\"King Charles visited London in April 2023 to attend a meeting with NHS officials.\")\n",
        "for key, value in results.items():\n",
        "    print(f\"{key}:\\n{value}\\n\")"
      ],
      "metadata": {
        "id": "iQWr8G92LtbO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f4d662e-d0b8-4476-cb61-bb744533c5ae"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "original_tokens:\n",
            "['King', 'Charles', 'visited', 'London', 'in', 'April', '2023', 'to', 'attend', 'a', 'meeting', 'with', 'NHS', 'officials', '.']\n",
            "\n",
            "lowercase:\n",
            "['king', 'charles', 'visited', 'london', 'in', 'april', '2023', 'to', 'attend', 'a', 'meeting', 'with', 'nhs', 'officials', '.']\n",
            "\n",
            "no_punctuation:\n",
            "['king', 'charles', 'visited', 'london', 'in', 'april', '2023', 'to', 'attend', 'a', 'meeting', 'with', 'nhs', 'officials']\n",
            "\n",
            "no_stopwords:\n",
            "['king', 'charles', 'visited', 'london', 'april', '2023', 'attend', 'meeting', 'nhs', 'officials']\n",
            "\n",
            "stems:\n",
            "['king', 'charl', 'visit', 'london', 'april', '2023', 'attend', 'meet', 'nh', 'offici']\n",
            "\n",
            "lemmata:\n",
            "['king', 'charles', 'visited', 'london', 'april', '2023', 'attend', 'meeting', 'nh', 'official']\n",
            "\n",
            "named_entities:\n",
            "[('Charles', 'PERSON'), ('London', 'GPE'), ('April 2023', 'DATE'), ('NHS', 'ORG')]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "😃😐😠 Sentiment Analysis – Reading the mood  \n",
        "We’ll use TextBlob, which is beginner-friendly and built on top of NLTK. It gives us two numbers:  \n",
        "  \n",
        "Polarity (from –1 to +1): negative → positive  \n",
        "  \n",
        "Subjectivity (from 0 to 1): fact → opinion"
      ],
      "metadata": {
        "id": "2fMIsQ7PL44m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "text = \"I absolutely love how easy this is to use!\"\n",
        "blob = TextBlob(text)\n",
        "\n",
        "print(\"Polarity:\", blob.sentiment.polarity)\n",
        "print(\"Subjectivity:\", blob.sentiment.subjectivity)\n"
      ],
      "metadata": {
        "id": "lk7EX251L7hq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1dddd87c-625f-495f-cd41-9bb124f0504c"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Polarity: 0.5208333333333334\n",
            "Subjectivity: 0.7166666666666667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Okay, that's a lot of NLP basics!\n",
        "\n",
        "# Now go back through and insert different sentences to see how that changes the output"
      ],
      "metadata": {
        "id": "hhnqosUEMAN-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Once you're satisfied, start looking at tokenisation:\n",
        "https://platform.openai.com/tokenizer  \n",
        "Type sentences into the box, looking at the output"
      ],
      "metadata": {
        "id": "zuhcol6RMRr1"
      }
    }
  ]
}